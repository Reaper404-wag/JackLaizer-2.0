import requests
from bs4 import BeautifulSoup
import json
import logging
from typing import Dict, List, Optional
import re
import time
import random
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.retry import Retry

class GOSTParser:
    def __init__(self, cache_file='gost_cache.json'):
        self.base_url = "https://docs.cntd.ru"
        self.search_url = f"{self.base_url}/search"
        self.cache_file = cache_file
        self.cache = self._load_cache()
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s: %(message)s',
            handlers=[
                logging.FileHandler('gost_parser.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)

        # –°–æ–∑–¥–∞–Ω–∏–µ —Å–µ—Å—Å–∏–∏ —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏
        self.session = requests.Session()
        retries = Retry(
            total=3,
            backoff_factor=0.5,
            status_forcelist=[500, 502, 503, 504]
        )
        self.session.mount('https://', HTTPAdapter(max_retries=retries))

    def _load_cache(self) -> Dict:
        try:
            with open(self.cache_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            return {}

    def _save_cache(self):
        with open(self.cache_file, 'w', encoding='utf-8') as f:
            json.dump(self.cache, f, ensure_ascii=False, indent=2)

    def _get_headers(self) -> Dict:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–ª—É—á–∞–π–Ω—ã—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –¥–ª—è –æ–±—Ö–æ–¥–∞ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫"""
        user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36'
        ]
        return {
            'User-Agent': random.choice(user_agents),
            'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Referer': 'https://www.google.com/'
        }

    def search_gost(self, query: str, max_results: int = 5) -> List[Dict]:
        """–ü–æ–∏—Å–∫ –ì–û–°–¢–æ–≤ –ø–æ –∑–∞–ø—Ä–æ—Å—É —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π"""
        self.logger.info(f"–ù–∞—á–∞–ª–æ –ø–æ–∏—Å–∫–∞ –ì–û–°–¢–æ–≤ –ø–æ –∑–∞–ø—Ä–æ—Å—É: {query}")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞
        if query in self.cache:
            self.logger.info("–í–æ–∑–≤—Ä–∞—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–∑ –∫—ç—à–∞")
            return self.cache[query]

        try:
            # –°–ª—É—á–∞–π–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–∏ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è
            time.sleep(random.uniform(1, 3))

            params = {
                'query': query,
                'type': 'normative_document'
            }
            
            response = self.session.get(
                self.search_url, 
                params=params, 
                headers=self._get_headers(),
                timeout=10
            )
            
            # –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            results = []

            for item in soup.select('.search-result')[:max_results]:
                try:
                    title = item.select_one('.search-result__title').text.strip()
                    link = self.base_url + item.select_one('a')['href']
                    
                    # –î–µ—Ç–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ì–û–°–¢–∞
                    gost_details = self._parse_gost_details(link)
                    
                    result = {
                        'title': title,
                        'link': link,
                        **gost_details
                    }
                    results.append(result)
                except Exception as item_error:
                    self.logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —ç–ª–µ–º–µ–Ω—Ç–∞: {item_error}")

            # –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            self.cache[query] = results
            self._save_cache()

            self.logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
            return results

        except requests.RequestException as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –ì–û–°–¢–∞: {e}")
            return []

    def _parse_gost_details(self, url: str) -> Dict:
        """–ü–∞—Ä—Å–∏–Ω–≥ –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ì–û–°–¢–µ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π"""
        try:
            response = self.session.get(
                url, 
                headers=self._get_headers(),
                timeout=10
            )
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–¥–∞ –ì–û–°–¢–∞
            code_match = re.search(r'–ì–û–°–¢\s+[\w.-]+', soup.text)
            gost_code = code_match.group(0) if code_match else "–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω"
            
            # –ü–æ–∏—Å–∫ –æ–ø–∏—Å–∞–Ω–∏—è
            description_elem = soup.select_one('.document-description')
            description = description_elem.text.strip() if description_elem else "–û–ø–∏—Å–∞–Ω–∏–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç"
            
            return {
                'code': gost_code,
                'description': description[:500]  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª–∏–Ω—ã –æ–ø–∏—Å–∞–Ω–∏—è
            }

        except requests.RequestException as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –¥–µ—Ç–∞–ª–µ–π –ì–û–°–¢–∞: {e}")
            return {}

def main():
    parser = GOSTParser()
    
    # –ü—Ä–∏–º–µ—Ä—ã –ø–æ–∏—Å–∫–∞
    test_queries = [
        "—Å—Ö–µ–º—ã –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤",
        "–±–∏–±–ª–∏–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∞—è —Å—Å—ã–ª–∫–∞",
        "–ø—Ä–æ–≥—Ä–∞–º–º–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è"
    ]

    for query in test_queries:
        print(f"\nüîç –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –¥–ª—è '{query}':")
        results = parser.search_gost(query)
        
        for result in results:
            print(f"–ù–∞–∑–≤–∞–Ω–∏–µ: {result.get('title', '–ù/–î')}")
            print(f"–ö–æ–¥: {result.get('code', '–ù/–î')}")
            print(f"–°—Å—ã–ª–∫–∞: {result.get('link', '–ù/–î')}")
            print(f"–û–ø–∏—Å–∞–Ω–∏–µ: {result.get('description', '–ù/–î')[:200]}...\n")
        
        time.sleep(1)  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏

if __name__ == "__main__":
    main()
